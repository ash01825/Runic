{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T08:06:26.528828Z",
     "start_time": "2025-10-16T08:05:57.969646Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "# Load the same sentence-transformer model used in our Lambda for consistency.\n",
    "print(\"Loading the sentence-transformer model 'all-MiniLM-L6-v2'...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/runic/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sentence-transformer model 'all-MiniLM-L6-v2'...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:06:39.691281Z",
     "start_time": "2025-10-16T08:06:39.680704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_and_chunk_markdown(doc):\n",
    "    \"\"\"Parses a markdown document, splitting it into chunks with section metadata.\"\"\"\n",
    "    chunks = []\n",
    "    lines = doc['content'].split('\\n')\n",
    "\n",
    "    current_section = \"Introduction\" # Default section\n",
    "    current_chunk_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Check for a markdown header (e.g., \"## üîç Symptoms\")\n",
    "        header_match = re.match(r'^##\\s+(.*)', line)\n",
    "        if header_match:\n",
    "            # If we find a new header, save the previous chunk\n",
    "            if current_chunk_lines:\n",
    "                chunks.append({\n",
    "                    'parent_doc': doc['name'],\n",
    "                    'section': current_section,\n",
    "                    'content': \"\\n\".join(current_chunk_lines).strip()\n",
    "                })\n",
    "                current_chunk_lines = [] # Reset for the new section\n",
    "\n",
    "            # Update the current section title (stripping the emoji)\n",
    "            current_section = header_match.group(1).split(' ', 1)[-1].strip()\n",
    "\n",
    "        # Add non-empty lines to the current chunk\n",
    "        if line.strip():\n",
    "            current_chunk_lines.append(line)\n",
    "\n",
    "    # Add the last remaining chunk\n",
    "    if current_chunk_lines:\n",
    "        chunks.append({\n",
    "            'parent_doc': doc['name'],\n",
    "            'section': current_section,\n",
    "            'content': \"\\n\".join(current_chunk_lines).strip()\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_generic_text(doc):\n",
    "    \"\"\"Splits generic text (like logs or JSON) into simple, overlapping chunks.\"\"\"\n",
    "    # For non-markdown files, we can just treat them as single-section documents\n",
    "    return [{\n",
    "        'parent_doc': doc['name'],\n",
    "        'section': 'Log Content',\n",
    "        'content': doc['content']\n",
    "    }]\n",
    "\n",
    "\n",
    "def load_and_chunk_documents(path):\n",
    "    \"\"\"Loads all documents and processes them into a unified list of chunks.\"\"\"\n",
    "    all_chunks = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.log', '.json', '.txt', '.md')):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        doc = {'name': file_name, 'content': f.read()}\n",
    "\n",
    "                        if file_name.endswith('.md'):\n",
    "                            chunks = parse_and_chunk_markdown(doc)\n",
    "                        else:\n",
    "                            chunks = chunk_generic_text(doc)\n",
    "\n",
    "                        all_chunks.extend(chunks)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading or chunking {file_path}: {e}\")\n",
    "    return all_chunks\n",
    "\n",
    "# --- Execute the loading and chunking ---\n",
    "print(\"\\nLoading, parsing, and chunking all documents from '../data'...\")\n",
    "all_chunks = load_and_chunk_documents('../data')\n",
    "print(f\"Successfully created {len(all_chunks)} chunks from the document base.\")"
   ],
   "id": "b032244d9a3d876b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading, parsing, and chunking all documents from '../data'...\n",
      "Successfully created 70 chunks from the document base.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:06:56.220674Z",
     "start_time": "2025-10-16T08:06:54.942477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nCreating embeddings for all chunks... (This may take a moment)\")\n",
    "chunk_contents = [chunk['content'] for chunk in all_chunks]\n",
    "\n",
    "# We encode the content of each chunk, not the whole document\n",
    "chunk_embeddings = model.encode(chunk_contents, convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings created successfully. Vector shape: {chunk_embeddings.shape}\")"
   ],
   "id": "72109709062c4f1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating embeddings for all chunks... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created successfully. Vector shape: (70, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:07:16.488262Z",
     "start_time": "2025-10-16T08:07:16.483922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The dimension 'd' is the size of our embeddings (384 for 'all-MiniLM-L6-v2')\n",
    "d = chunk_embeddings.shape[1]\n",
    "\n",
    "print(\"\\nBuilding the FAISS index from chunk embeddings...\")\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(np.array(chunk_embeddings, dtype='float32')) # Ensure dtype is float32 for FAISS\n",
    "print(f\"FAISS index built. Total vectors in index: {index.ntotal}\")"
   ],
   "id": "cdeaef4ce865d749",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the FAISS index from chunk embeddings...\n",
      "FAISS index built. Total vectors in index: 70\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:07:26.084789Z",
     "start_time": "2025-10-16T08:07:25.803612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, k=3):\n",
    "    \"\"\"Searches the FAISS index and returns the top k most relevant CHUNKS.\"\"\"\n",
    "    print(f\"\\n================================================================\")\n",
    "    print(f\"Searching for top {k} chunks matching query: '{query}'\")\n",
    "    print(f\"================================================================\")\n",
    "\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding, dtype='float32'), k)\n",
    "\n",
    "    print(\"\\n--- Search Results ---\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        # Retrieve the chunk using the index\n",
    "        retrieved_chunk = all_chunks[idx]\n",
    "\n",
    "        print(f\"\\n{i+1}. Document: '{retrieved_chunk['parent_doc']}' | Section: '{retrieved_chunk['section']}' (Score: {distances[0][i]:.4f})\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        # Print the actual content of the chunk\n",
    "        print(retrieved_chunk['content'])\n",
    "\n",
    "# --- Execute Test Queries ---\n",
    "search(\"The auth-service has high CPU usage and is exhausted\")\n",
    "search(\"I'm getting database connection timeouts and latency spikes from the payment gateway\")\n",
    "search(\"My search-engine service is in a crash loop after the last deployment and is unavailable\")"
   ],
   "id": "9ec86882df20c5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "Searching for top 3 chunks matching query: 'The auth-service has high CPU usage and is exhausted'\n",
      "================================================================\n",
      "\n",
      "--- Search Results ---\n",
      "\n",
      "1. Document: 'high_cpu_restart.md' | Section: 'Resolution Steps' (Score: 0.8212)\n",
      "--------------------------------------------------\n",
      "## üõ†Ô∏è Resolution Steps\n",
      "1.  **Acknowledge Incident:** Update the incident status in the `OpsFlowIncidents` table to `INVESTIGATING`.\n",
      "2.  **Immediate Mitigation:** Perform a rolling restart of the affected service (e.g., `auth-service`). This is the fastest way to recover from a stuck process.\n",
      "3.  **Monitor Post-Restart:** Closely observe the `cpuUsagePercent` metric for 5 minutes after the restart.\n",
      "4.  **Escalate if Unresolved:** If the CPU spikes again immediately, the issue is likely not a transient stuck process. Escalate to the on-call engineer and consider a rollback of the last deployment.\n",
      "\n",
      "2. Document: 'incident_001_cpu_spike.json' | Section: 'Log Content' (Score: 0.8946)\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"timestamp\": \"2025-10-16T10:06:03Z\",\n",
      "  \"service\": \"auth-service\",\n",
      "  \"level\": \"ERROR\",\n",
      "  \"msg\": \"High resource warning; triggering ResourceExhaustion event\",\n",
      "  \"eventType\": \"ResourceExhaustion\",\n",
      "  \"details\": {\n",
      "    \"cpu_utilization_percent\": 94.3,\n",
      "    \"p99_latency_ms\": 2100,\n",
      "    \"task_id\": \"b4a2e1f-8c7d\",\n",
      "    \"error\": \"Request processing time exceeded 5000ms\"\n",
      "  }\n",
      "}\n",
      "\n",
      "3. Document: 'high_cpu_restart.md' | Section: 'Incident Symptoms' (Score: 0.9690)\n",
      "--------------------------------------------------\n",
      "## üîç Incident Symptoms\n",
      "- An incident is flagged as an anomaly with a score of **1.0 or higher**.\n",
      "- The `metricValue` (CPU Usage) is **> 90%**.\n",
      "- The `eventType` in the alert payload is `ResourceExhaustion`.\n",
      "- The `service` field identifies a specific component, like `auth-service`, as the source.\n",
      "- P99 latency for the service is significantly elevated.\n",
      "\n",
      "================================================================\n",
      "Searching for top 3 chunks matching query: 'I'm getting database connection timeouts and latency spikes from the payment gateway'\n",
      "================================================================\n",
      "\n",
      "--- Search Results ---\n",
      "\n",
      "1. Document: 'database_latency.md' | Section: 'Resolution Steps' (Score: 1.0495)\n",
      "--------------------------------------------------\n",
      "## üõ†Ô∏è Resolution Steps\n",
      "1.  **Identify Long-Running Queries:** Use the database's native tools (e.g., `pg_stat_activity` for Postgres) to find and analyze long-running queries.\n",
      "2.  **Terminate Offending Query:** If a single query is identified as the cause, terminate its process ID to immediately relieve pressure.\n",
      "3.  **Restart Connection Pool:** If no single query is at fault, restart the application services (e.g., `payment-gateway`) to reset their database connection pools.\n",
      "4.  **Failover Database:** As a last resort for an unresponsive database, initiate a manual failover to a read replica.\n",
      "\n",
      "2. Document: 'database_latency.md' | Section: 'Introduction' (Score: 1.0564)\n",
      "--------------------------------------------------\n",
      "# High Database Query Latency\n",
      "\n",
      "3. Document: 'database_latency.md' | Section: 'Incident Symptoms' (Score: 1.1038)\n",
      "--------------------------------------------------\n",
      "## üîç Incident Symptoms\n",
      "- The `eventType` is `LatencySpike` or `Timeouts`.\n",
      "- The `service` experiencing the issue is a database like `inventory-db`, or a service that depends on it.\n",
      "- Application-level metrics show a sharp increase in the duration of database transactions.\n",
      "- CloudWatch RDS metrics show high `DBLoad` and a low number of `FreeableMemory`.\n",
      "\n",
      "================================================================\n",
      "Searching for top 3 chunks matching query: 'My search-engine service is in a crash loop after the last deployment and is unavailable'\n",
      "================================================================\n",
      "\n",
      "--- Search Results ---\n",
      "\n",
      "1. Document: 'service_crash_loop.md' | Section: 'Introduction' (Score: 0.8532)\n",
      "--------------------------------------------------\n",
      "# Service Crash Loop\n",
      "\n",
      "2. Document: 'service_crash_loop.md' | Section: 'Resolution Steps' (Score: 0.8839)\n",
      "--------------------------------------------------\n",
      "## üõ†Ô∏è Resolution Steps\n",
      "1.  **Immediate Rollback:** The highest-probability cause is a bad deployment. Immediately initiate a rollback to the previously known good version of the service.\n",
      "2.  **Inspect Logs for Fatal Errors:** While the rollback is in progress, inspect the service's logs for the fatal error message that occurs just before it crashes.\n",
      "3.  **Check Configuration:** Validate that all required environment variables and secrets are correctly mounted and accessible to the service.\n",
      "4.  **Review Resource Limits:** Check the service's CPU and memory allocation. If a memory leak is suspected, temporarily increase the memory limit.\n",
      "\n",
      "3. Document: 'incident_004_crash_loop.json' | Section: 'Log Content' (Score: 1.0003)\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"alertName\": \"ServiceUnhealthy\",\n",
      "  \"source\": \"ECS\",\n",
      "  \"severity\": \"CRITICAL\",\n",
      "  \"timestamp\": \"2025-10-14T04:32:45Z\",\n",
      "  \"eventType\": \"ServiceDown\",\n",
      "  \"serviceName\": \"search-engine\",\n",
      "  \"clusterName\": \"opsflow-prod-cluster\",\n",
      "  \"reason\": \"Service has been restarting continuously.\",\n",
      "  \"details\": {\n",
      "    \"taskDefinition\": \"search-engine:v1.2.1\",\n",
      "    \"restartCount\": 6,\n",
      "    \"timeWindowMinutes\": 10,\n",
      "    \"lastError\": \"Container health check failed\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
